<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Random Notes!</title>
    <link>https://azmfaridee.github.io/</link>
    <description>Recent content on Random Notes!</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://azmfaridee.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading List</title>
      <link>https://azmfaridee.github.io/post/2021/10/15/reading-list/</link>
      <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://azmfaridee.github.io/post/2021/10/15/reading-list/</guid>
      <description>ICML, July 2021 ICLR 2021, May 2021  Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data [paper] Read First Read Later  NeurIPS 2021 NeurIPS 2020  Unsupervised Data Augmentation for Consistency Training [paper|code]    Selected list of papers (and possible summaries) on domain adaptation, self-supervision, optimal transport, few-shot learning, generative models, adversarial models, disentanglement
ICML, July 2021 List of papers
 Unbalanced minibatch Optimal Transport; applications to Domain Adaptation  Sequential Domain Adaptation by Synthesizing Distributionally Robust Experts  LAMDA: Label Matching Deep Domain Adaptation Representation Subspace Distance for Domain Adaptation Regression Dataset Dynamics via Gradient Flows in Probability Space Optimizing Black-box Metrics with Iterative Example Weighting A Theory of Label Propagation for Subpopulation Shift Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization Aggregating From Multiple Target-Shifted Sources f-Domain Adversarial Learning: Theory and Algorithms Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation A Discriminative Technique for Multiple-Source Adaptation KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation Zoo-Tuning: Adaptive Transfer from A Zoo of Models Self-Tuning for Data-Efficient Deep Learning Continual Learning in the Teacher-Student Setup: Impact of Task Similarity Learning Bounds for Open-Set Learning Barlow Twins: Self-Supervised Learning via Redundancy Reduction   ICLR 2021, May 2021 List of all papers</description>
    </item>
    
    <item>
      <title>ML Notes</title>
      <link>https://azmfaridee.github.io/post/2020/10/19/ml-notes/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://azmfaridee.github.io/post/2020/10/19/ml-notes/</guid>
      <description>KNN From Scratch  Speeding Up Multiple Test Cases    KNN From Scratch I have often been asked to implement a KNN classifier in a few interviews so I thought I’d write down my simplest implementation on MNIST and talk about various ways we can speed it up.
First off, import our python libraries
import mnist import numpy as np from scipy.stats import mode from sklearn.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://azmfaridee.github.io/about/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://azmfaridee.github.io/about/</guid>
      <description>About Me Publications  Primary Research Area Others  Experience Education Services   About Me I’m a final year Ph.D. candidate in Information Systems at the University of Maryland, Baltimore County. I work with Dr. Nirmalya Roy in the Mobile, Pervasive, and Sensor Computing (MPSC) Lab.
My current research is focused on building scalable machine learning models that are robust against domain and category shifts with minimal-to-no extra label information.</description>
    </item>
    
    <item>
      <title>Transfer Learning Notes</title>
      <link>https://azmfaridee.github.io/post/2020/10/15/transfer-learning/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://azmfaridee.github.io/post/2020/10/15/transfer-learning/</guid>
      <description>Label, Domain and Tasks: A Definition Domain Adaptation and Transfer Learning References   Label, Domain and Tasks: A Definition Here is a brief definition of label, domain and the tasks (Yang et al. 2020).
A domain $\mathbb{D}$ consists of:
 a feature space $\mathscr{X}$ and a marginal probability distribution $\mathbb{P}^X$, where each input instance $x \in \mathscr{X}$.  Two domains are different when they have different feature space or different marginal probability distributions.</description>
    </item>
    
  </channel>
</rss>
